{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"regenie regenie is a C++ program for whole genome regression modelling of large genome-wide association studies . It is developed and supported by a team of scientists at the Regeneron Genetics Center. The method has the following properties It works on quantitative and binary traits, including binary traits with unbalanced case-control ratios It can process multiple phenotypes at once For binary traits it supports Firth logistic regression and an SPA test It can perform gene/region-based burden tests It is fast and memory efficient \ud83d\udd25 It supports the BGEN , PLINK bed/bim/fam and PLINK2 pgen/pvar/psam genetic data formats It is ideally suited for implementation in Apache Spark (see GLOW ) Citation Joelle Mbatchou, Leland Barnard, Joshua Backman, Anthony Marcketta, Jack A. Kosmicki, Andrey Ziyatdinov, Christian Benner, Colm O'Dushlaine, Mathew Barber, Boris Boutkov, Lukas Habegger, Manuel Ferreira, Jeffrey Reid, Goncalo Abecasis, Evan Maxwell, Jonathan Marchini. (2020) Computationally efficient whole genome regression for quantitative and binary traits [BioRxiv pre-print] License regenie is distributed under an MIT license . Contact If you have any questions about regenie please contact If you want to submit a issue concerning the software please do so using the regenie Github repository .","title":"Home"},{"location":"#regenie","text":"regenie is a C++ program for whole genome regression modelling of large genome-wide association studies . It is developed and supported by a team of scientists at the Regeneron Genetics Center. The method has the following properties It works on quantitative and binary traits, including binary traits with unbalanced case-control ratios It can process multiple phenotypes at once For binary traits it supports Firth logistic regression and an SPA test It can perform gene/region-based burden tests It is fast and memory efficient \ud83d\udd25 It supports the BGEN , PLINK bed/bim/fam and PLINK2 pgen/pvar/psam genetic data formats It is ideally suited for implementation in Apache Spark (see GLOW ) Citation Joelle Mbatchou, Leland Barnard, Joshua Backman, Anthony Marcketta, Jack A. Kosmicki, Andrey Ziyatdinov, Christian Benner, Colm O'Dushlaine, Mathew Barber, Boris Boutkov, Lukas Habegger, Manuel Ferreira, Jeffrey Reid, Goncalo Abecasis, Evan Maxwell, Jonathan Marchini. (2020) Computationally efficient whole genome regression for quantitative and binary traits [BioRxiv pre-print]","title":"regenie"},{"location":"#license","text":"regenie is distributed under an MIT license .","title":"License"},{"location":"#contact","text":"If you have any questions about regenie please contact If you want to submit a issue concerning the software please do so using the regenie Github repository .","title":"Contact"},{"location":"faq/","text":"Frequently asked questions What block size to use in step 1? We recommend to use blocks of size 1000 as we have observed that it leads to a reasonable number of ridge predictors at level 1 (e.g. 2,500 with 500K SNPs used and the default regenie parameters) and have noticed little change in the final predictions when varying the block size. How many variants to use in step 1? We recommend to use a smaller set of about 500K directly genotyped SNPs in step 1, which should be sufficient to capture genome-wide polygenic effects. Note that using too many SNPs in Step 1 (e.g. 1M) can lead to a high computational burden due to the resulting higher number of predictors in the level 1 models. What do I do if I get the error \"Uh-oh, SNP XX has low variance (=XX)\" in step 1? This is due to variants with very low minor allele count (MAC) being included in step 1. To avoid this, you should use a MAC filter to remove such variants in a pre-processing step before running Regenie. For example, in PLINK2 you would use the --mac option and obtain a list of variants that pass the MAC filter (note that if you are using --keep/--remove in Regenie, you should also use it in the PLINK2 command) plink2 \\ --bfile my_bed_file \\ --mac 100 \\ --write-snplist \\ --out snps_pass You would then use the output file in regenie as --extract snps_pass.snplist (and this would avoid having to make a new genotype file). Can regenie be run on small sample sizes? For quantitative traits, we have not obtained issues running regenie on small data sets. For binary traits, we have obtained successful runs of regenie (step 1 and 2) on data sets with as little as 300 samples. A few factors to consider: Convergence issues may occur in step 1 (all the more if a trait is highly unbalanced) - see below Similarly, convergence issues may occur in step 2 when using Firth approximation - see below Note: we have found that regenie can get conservative in more extreme relatedness scenarios so we recommend not to use it for smaller cohorts with high amounts of relatedness like founder populations where exact mixed-model methods can be used What to do if Step 1 of regenie failed for a binary trait when fitting the penalized logsitic regression model? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. If using K-fold CV, switch to LOOCV (option --loocv ) to increase the size of the sample used to fit the model LOOCV is now used by default when the sample size is less than 5,000 If it is due to quasi-separation (i.e. Var(Y)=0 occurred in model fitting), either increase the sample size using LOOCV or increase the MAF threshold for variants included in step 1 analysis What to do if Step 2 of regenie fails when fitting the null model for the approximate Firth correction? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. We have implemented the same measures as in the logistf function in R to avoid convergence issues, which include the use of a step size threshold when performing a Newton step. We first try fitting the model with a step size threshold that is more liberal (=25) as well as a maximum number of iterations of 1,000 and if convergence fails, we retry the model fit using a more stringent step size threshold (=5) and a higher threshold for the number of iterations (=5,000), which will slow down convergence. The user can also specify a maximum step size threshold using --maxstep-null (use value 5) as well as increase the maximum number of iterations using --maxiter-null (use value 5000). In that case, no retries are perfomed if convergence fails. We recommend to test chromosomes separately (using --chr ) as these parameters may need to be altered when fitting the null model for each chromosome","title":"F.A.Q."},{"location":"faq/#frequently-asked-questions","text":"What block size to use in step 1? We recommend to use blocks of size 1000 as we have observed that it leads to a reasonable number of ridge predictors at level 1 (e.g. 2,500 with 500K SNPs used and the default regenie parameters) and have noticed little change in the final predictions when varying the block size. How many variants to use in step 1? We recommend to use a smaller set of about 500K directly genotyped SNPs in step 1, which should be sufficient to capture genome-wide polygenic effects. Note that using too many SNPs in Step 1 (e.g. 1M) can lead to a high computational burden due to the resulting higher number of predictors in the level 1 models. What do I do if I get the error \"Uh-oh, SNP XX has low variance (=XX)\" in step 1? This is due to variants with very low minor allele count (MAC) being included in step 1. To avoid this, you should use a MAC filter to remove such variants in a pre-processing step before running Regenie. For example, in PLINK2 you would use the --mac option and obtain a list of variants that pass the MAC filter (note that if you are using --keep/--remove in Regenie, you should also use it in the PLINK2 command) plink2 \\ --bfile my_bed_file \\ --mac 100 \\ --write-snplist \\ --out snps_pass You would then use the output file in regenie as --extract snps_pass.snplist (and this would avoid having to make a new genotype file). Can regenie be run on small sample sizes? For quantitative traits, we have not obtained issues running regenie on small data sets. For binary traits, we have obtained successful runs of regenie (step 1 and 2) on data sets with as little as 300 samples. A few factors to consider: Convergence issues may occur in step 1 (all the more if a trait is highly unbalanced) - see below Similarly, convergence issues may occur in step 2 when using Firth approximation - see below Note: we have found that regenie can get conservative in more extreme relatedness scenarios so we recommend not to use it for smaller cohorts with high amounts of relatedness like founder populations where exact mixed-model methods can be used What to do if Step 1 of regenie failed for a binary trait when fitting the penalized logsitic regression model? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. If using K-fold CV, switch to LOOCV (option --loocv ) to increase the size of the sample used to fit the model LOOCV is now used by default when the sample size is less than 5,000 If it is due to quasi-separation (i.e. Var(Y)=0 occurred in model fitting), either increase the sample size using LOOCV or increase the MAF threshold for variants included in step 1 analysis What to do if Step 2 of regenie fails when fitting the null model for the approximate Firth correction? This can occur when the sample size used to fit the model is small and/or if the trait is extremely unbalanced. We have implemented the same measures as in the logistf function in R to avoid convergence issues, which include the use of a step size threshold when performing a Newton step. We first try fitting the model with a step size threshold that is more liberal (=25) as well as a maximum number of iterations of 1,000 and if convergence fails, we retry the model fit using a more stringent step size threshold (=5) and a higher threshold for the number of iterations (=5,000), which will slow down convergence. The user can also specify a maximum step size threshold using --maxstep-null (use value 5) as well as increase the maximum number of iterations using --maxiter-null (use value 5000). In that case, no retries are perfomed if convergence fails. We recommend to test chromosomes separately (using --chr ) as these parameters may need to be altered when fitting the null model for each chromosome","title":"Frequently asked questions"},{"location":"install/","text":"Download The regenie source code is hosted on Github . Installation Note: regenie requires compilation with GCC version = 5.1 (on Linux) or Clang version =3.3 (on Mac OSX) Pre-compiled binaries Pre-compiled binaries are available in the Github repository . These are provided for Linux and Mac OSX computing environments and are statically linked. For the Linux binaries, user should have GLIBC version = 2.22 installed. Standard installation regenie requires the BGEN library so you will need to download and install that library. In the source code edit the BGEN_PATH variable in the Makefile to the BGEN library path. On the command line type make while in the main source code directory. This should produce the executable called regenie . regenie has been enhanced to allow for gzip compressed input (for phenotype/covariate files) and output (for association results files) using the Boost Iostream library. If this library is installed on the system, you should compile using make HAS_BOOST_IOSTREAM=1 . With Docker Alternatively, you can use a Docker image to run regenie . A guide to using docker is available on the Github page . With conda To install with conda, follow the directions here . Computing requirements We have tested regenie on 64-bit Linux and 64-bit Mac OSX computing environments. Note that for Mac OSX computing environments, compiling is done without OpenMP, as the library is not built-in by default and has to be installed separately. Memory usage In both Step 1 and Step 2 of a regenie run the genetic data file is read once, in blocks of SNPs, so at no point is the full dataset ever stored in memory. regenie uses a dimension reduction approach using ridge regression to produce a relatively small set of genetic predictors, that are then used to fit a whole-genome regression model. These genetic predictors are stored in memory by default, and can be relatively large if many phenotypes are stored at once. For example, if there are P phenotypes, M SNPs and N samples, and a block size of B SNPs is used with R ridge parameters, then regenie needs to store roughly N\\times M/B\\times R doubles per phenotype, which is 8Gb per phenotype when M=500,000, N=400,000, B =1,000,R=5 and 200Gb in total when P=25 . However, the --lowmem option can be used to avoid that memory usage, at negligible extra computational cost, by writing temporary files to disk. Threading regenie can take advantage of multiple cores using threading. The number of threads can be specified using the --threads option. regenie uses the Eigen library for efficient linear algebra operations and this uses threading where possible. For PLINK bed/bim/fam files, PLINK2 pgen/pvar/psam files, as well as BGEN v1.2 files with 8-bit encoding (format used for UK Biobank 500K imputed data), step 2 of regenie has been optimized by using multithreading through OpenMP .","title":"Install"},{"location":"install/#download","text":"The regenie source code is hosted on Github .","title":"Download"},{"location":"install/#installation","text":"Note: regenie requires compilation with GCC version = 5.1 (on Linux) or Clang version =3.3 (on Mac OSX)","title":"Installation"},{"location":"install/#pre-compiled-binaries","text":"Pre-compiled binaries are available in the Github repository . These are provided for Linux and Mac OSX computing environments and are statically linked. For the Linux binaries, user should have GLIBC version = 2.22 installed.","title":"Pre-compiled binaries"},{"location":"install/#standard-installation","text":"regenie requires the BGEN library so you will need to download and install that library. In the source code edit the BGEN_PATH variable in the Makefile to the BGEN library path. On the command line type make while in the main source code directory. This should produce the executable called regenie . regenie has been enhanced to allow for gzip compressed input (for phenotype/covariate files) and output (for association results files) using the Boost Iostream library. If this library is installed on the system, you should compile using make HAS_BOOST_IOSTREAM=1 .","title":"Standard installation"},{"location":"install/#with-docker","text":"Alternatively, you can use a Docker image to run regenie . A guide to using docker is available on the Github page .","title":"With Docker"},{"location":"install/#with-conda","text":"To install with conda, follow the directions here .","title":"With conda"},{"location":"install/#computing-requirements","text":"We have tested regenie on 64-bit Linux and 64-bit Mac OSX computing environments. Note that for Mac OSX computing environments, compiling is done without OpenMP, as the library is not built-in by default and has to be installed separately.","title":"Computing requirements"},{"location":"install/#memory-usage","text":"In both Step 1 and Step 2 of a regenie run the genetic data file is read once, in blocks of SNPs, so at no point is the full dataset ever stored in memory. regenie uses a dimension reduction approach using ridge regression to produce a relatively small set of genetic predictors, that are then used to fit a whole-genome regression model. These genetic predictors are stored in memory by default, and can be relatively large if many phenotypes are stored at once. For example, if there are P phenotypes, M SNPs and N samples, and a block size of B SNPs is used with R ridge parameters, then regenie needs to store roughly N\\times M/B\\times R doubles per phenotype, which is 8Gb per phenotype when M=500,000, N=400,000, B =1,000,R=5 and 200Gb in total when P=25 . However, the --lowmem option can be used to avoid that memory usage, at negligible extra computational cost, by writing temporary files to disk.","title":"Memory usage"},{"location":"install/#threading","text":"regenie can take advantage of multiple cores using threading. The number of threads can be specified using the --threads option. regenie uses the Eigen library for efficient linear algebra operations and this uses threading where possible. For PLINK bed/bim/fam files, PLINK2 pgen/pvar/psam files, as well as BGEN v1.2 files with 8-bit encoding (format used for UK Biobank 500K imputed data), step 2 of regenie has been optimized by using multithreading through OpenMP .","title":"Threading"},{"location":"options/","text":"Getting started To run regenie , use the command ./regenie on the command line, followed by options and flags as needed. To get a full list of options use ./regenie --help The directory examples/ contains some small example files that are useful when getting started. A test run on a set of binary traits can be achieved by the following 2 commands. In Step 1 , the whole genome regression model is fit to the traits, and a set of genomic predictions are produced as output ./regenie \\ --step 1 \\ --bed example/example \\ --exclude example/snplist_rm.txt \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 100 \\ --bt --lowmem \\ --lowmem-prefix tmp_rg \\ --out fit_bin_out In Step 2 , a set of imputed SNPs are tested for association using a Firth logistic regression model ./regenie \\ --step 2 \\ --bgen example/example.bgen \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 200 \\ --bt \\ --firth --approx \\ --pThresh 0.01 \\ --pred fit_bin_out_pred.list \\ --out test_bin_out_firth One of the output files from these two commands is included in the example/ directory and you can check they are the same using the following command (the message should be printed out) cmp test_bin_out_firth_Y1.regenie example/example.test_bin_out_firth_Y1.regenie \\ echo Files are identical Basic options Input Option Argument Type Description --bgen, --bed, --pgen FILE Required Input genetic data file. Either BGEN file eg. file.bgen , or bed/bim/fam prefix that assumes file.bed , file.bim , file.fam exist, or pgen/pvar/psam prefix that assumes file.pgen , file.pvar , file.psam exist --sample FILE Optional Sample file corresponding to input BGEN file --ref-first FLAG Optional Specify to use the first allele as the reference allele for BGEN or PLINK bed/bim/fam file input [default is to use the last allele as the reference] --keep FILE Optional Inclusion file that lists individuals to retain in the analysis --remove FILE Optional Exclusion file that lists individuals to remove from the analysis --extract FILE Optional Inclusion file that lists IDs of variants to keep --exclude FILE Optional Exclusion file that lists IDs of variants to remove --phenoFile FILE Required Phenotypes file --phenoCol STRING Optional Use for each phenotype you want to include in the analysis --phenoColList STRING Optional Comma separated list of phenotypes to include in the analysis --covarFile FILE Optional Covariates file --covarCol STRING Optional Use for each covariate you want to include in the analysis --covarColList STRING Optional Comma separated list of covariates to include in the analysis --catCovarList STRING Optional Comma separated list of categorical covariates to include in the analysis --pred FILE Optional File containing predictions from Step 1 (see Overview). This is required for --step 2 Note: Parameter expansion can be used when specifying phenotypes/covariates (e.g. --covarCol PC{1:10} ). Genetic data file format regenie can read BGEN files, bed/bim/fam files or pgen/psam/pvar files in Step 1 and Step 2. The BGEN file format is described here . The bed/bim/fam file format is described here . The pgen/pvar/psam file format is described here . Tools useful for genetic data file format conversion are : PLINK , QCTOOL , BCFTOOLS . Step 2 of regenie can be sped up with BGEN files by using v1.2 format with 8 bits encoding (genotype file can be generated with PLINK2 using option --export bgen-1.2 'bits=8' ) as well as having an accompanying .bgi index file (a useful tool to create such file is bgenix which is part of the BGEN library). To include X chromosome genotypes in step 1 and/or step 2, males should be coded as diploid so that their genotypes are 0/2. This can be done in PLINK by setting the sex of all individuals to female before generating the genotype file. Chromosome values of 23 (for human analyses), X, XY, PAR1 and PAR2 are all acceptable and will be collapsed into a single chromosome. Sample inclusion/exclusion file format 2 2 7 7 . No header. Each line starts with individual FID IID. Space/tab separated. Samples listed in the file that are not in bgen/bed/pgen file are ignored. Variant inclusion/exclusion file format 20 31 . No header. Each line must start with variant ID (if there are additional columns, file must be space/tab separated). Variants listed in this file that are not in bgen/bed/pgen file are ignored. Covariate file format FID IID V1 V2 V3 1 1 1.46837294454993 1.93779743016325 0.152887004505393 2 2 -1.2234390803815 -1.63408619199948 -0.190201446835255 3 3 0.0711531925667286 0.0863906292357564 0.14254739715665 . Line 1 : Header with FID, IID and C covariate names. Followed by lines of C+2 values. Space/tab separated. Each line contains individual FID and IID followed by C covariate values. Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. No missing values are allowed. If --step 2 is specified, then the covariate file should be the same as that used in Step 1. Phenotype file format FID IID Y1 Y2 1 1 1.64818554321186 2.2765234736685 2 2 -2.67352013711554 -1.53680421614647 3 3 0.217542851471485 0.437289912695016 . Line 1 : Header with FID, IID and P phenotypes names. Followed by lines of P+2 values. Space/tab separated. Each line contains individual FID and IID followed by P phenotype values (for binary traits, must be coded as 0=control, 1=case, NA=missing unless using --1 ). Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. Missing values must be coded as NA. With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait . In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes, use option --strict in Step 1 and 2. Predictions file format Running --step 1 --out foo will produce A set of files containing genomic predictions for each phenotype from Step 1 (see Output section below). A file called foo_pred.list listing the locations of the prediction files. The file list is needed as an input file when using --step 2 via the --pred option. It has one line per phenotype (in any order) that specifies the name of the phenotype and its corresponding prediction file name. Each phenotype must have exactly one prediction file and phenotype names must match with those in the phenotype file. Phenotypes in this file not included in the analysis are ignored. Each prediction file contains the genetic predictions for the phenotype (space separated). Line 1 starts with 'FID_IID' followed by $N$ sample identifiers. It is followed by 23 lines containing the genetic predictions for each chromosome (sex chromosomes are collapsed into chromosome 23). More specifically, each line has $N+1$ values which are the chromosome number followed by the $N$ leave-one chromosome out (LOCO) predictions for each individual. Samples in this file not in the bed/pgen/bgen input file are ignored. Genotyped samples not present in this file will be ignored in the analysis of the corresponding trait. Samples with missing LOCO predictions must have their corresponding phenotype value set to missing. Options Option Argument Type Description --step INT Required specify step for the regenie run (see Overview) [argument can be 1 or 2 ] --bt FLAG Optional specify that traits are binary with 0=control,1=case,NA=missing (default is quantitative) -1,--cc12 FLAG Optional specify to use 1/2/NA encoding for binary traits (1=control,2=case,NA=missing) --bsize INT Required size of the genotype blocks --cv INT Optional number of cross validation (CV) folds [default is 5] --loocv FLAG Optional flag to use leave-one out cross validation --lowmem FLAG Optional flag to reduce memory usage by writing level 0 predictions to disk (details below). This is very useful if the number of traits is large (e.g. greater than 10) --lowmem-prefix FILE PREFIX Optional prefix where to temporarily write the level 0 predictions --split-l0 PREFIX,N Optional split level 0 across N jobs and set prefix of output files of level 0 predictions --run-l0 FILE,K Optional run level 0 for job K in --run-l1 FILE Optional run level 1 specifying the master file from '--split-l0' --keep-l0 FLAG Optional avoid deleting the level 0 predictions written on disk after fitting the level 1 models --print-prs FLAG Optional flag to print whole genome predictions (i.e. PRS) without using LOCO scheme --force-step1 FLAG Optional flag to run step 1 when 1M variants are used (not recommened) --nb INT Optional number of blocks (determined from block size if not provided) --strict FLAG Optional flag to removing samples with missing data at any of the phenotypes --ignore-pred FLAG Optional skip reading the file specified by --pred (corresponds to simple linear/logistic regression) --use-prs FLAG Optional flag to use whole genome PRS in --pred (this is output in step 1 when using --print-prs ) --gz FLAG Optional flag to output files in compressed gzip format (LOCO prediction files in step 1 and association results files in step 2) [this only works when compiling with Boost Iostream library (see Install tab)] . --force-impute FLAG Optional flag to keep and impute missing observations for QTs in step 2 --write-samples FLAG Optional flag to write sample IDs for those kept in the analysis for each trait in step 2 --print-pheno FLAG Optional flag to write phenotype name in the first line of the sample ID files when using --write-samples --firth FLAG Optional specify to use Firth likelihood ratio test (LRT) as fallback for p-values less than threshold --approx FLAG Optional flag to use approximate Firth LRT for computational speedup (only works when option --firth is used) --firth-se FLAG Optional flag to compute SE based on effect size and LRT p-value when using Firth correction (instead of based on Hessian of unpenalized log-likelihood) --spa FLAG Optional specify to use Saddlepoint approximation as fallback for p-values less than threshold --pThresh FLOAT Optional P-value threshold below which to apply Firth/SPA correction [default is 0.05] --test STRING Optional specify to carry out dominant or recessive test [default is additive; argument can be dominant or recessive ] --chr INT Optional specify which chromosomes to test in step 2 (use for each chromosome to include) --chrList STRING Optional Comma separated list of chromosomes to test in step 2 --range STRING Optional specify chromosome region for variants to test in step 2 [format=CHR:MINPOS-MAXPOS] --minMAC FLOAT Optional flag to specify the minimum minor allele count (MAC) when testing variants [default is 5]. Variants with lower MAC are ignored. --minINFO FLOAT Optional flag to specify the minimum imputation info score (IMPUTE/MACH R^2) when testing variants. Variants with lower info score are ignored. --nauto INT Optional number of autosomal chromosomes (for non-human studies) [default is 22] --maxCatLevels INT Optional maximum number of levels for categorical covariates (for non-human studies) [default is 10] --niter INT Optional maximum number of iterations for logistic regression [default is 30] --maxstep-null INT Optional maximum step size for logistic model with Firth penalty under the null [default is 25] --maxiter-null INT Optional maximum number of iterations for logistic model with Firth penalty under the null [default is 1000] --threads INT Optional number of computational threads to use [default=all] --debug FLAG Optional debug flag (for use by developers) --verbose FLAG Optional verbose screen output --help FLAG Optional Prints usage and options list to screen When step 1 of regenie is run in low memory mode (i.e. using --lowmem ), temporary files are created on disk (using --lowmem-prefix tmp_prefix determines where the files are written [as in tmp_prefix_l0_Y1 ,..., tmp_prefix_l0_YP for P phenotypes]). If the prefix is not specified, the default is to use the prefix specified by --out (see below). These are automatically deleted at the end of the program (unless the run was not successful in which case the user would need to delete the files) See the Wiki page for more details on how to run the level 0 models for Step 1 of regenie in parallel. Output Option Argument Type Description --out FILE PREFIX Required Output files that depends on --step A log file file.log of the output is generated. Using --step 1 --out file For the P phenotypes, files file_1.loco ,..., file_P.loco are output with the per-chromosome LOCO predictions as rows of the files. If option --gz was used, the files will be compressed in gzip format and have extension .loco.gz . Genotyped individuals specified using option --remove are excluded from this file. Individuals with missing phenotype values kept in the analysis are included in the file and have their predictions set to missing. The list of blup files needed for step 2 (association testing) is written to file_pred.list . If using --print-prs , files files_1.prs ,..., files_P.prs will be written with the whole genome predictions (i.e. PRS) without using LOCO scheme (similar format as the .loco files). The list of these files is written to file_prs.list and can be used in step 2 with --pred and specifying flag --use-prs . Note that as these are not obtained using a LOCO scheme, association tests could suffer from proximal contamination. Using --step 2 --out file By default, results are written in separate files for each phenotype file_ phenotype1_name .regenie,...,file_ phenotypeP_name .regenie . Each file has one line per SNP along with a header line. If option --gz was used, the files will be compressed in gzip format and have extension .regenie.gz . The entries of each row specify chromosome, posistion, ID, reference allele (allele 0), alternative allele (allele 1), frequency of the alternative allele, sample size and the test performed (additive/dominant/recessive). With BGEN/PGEN files with dosages, the imputation INFO score is provided (IMPUTE info score for BGEN and Mach Rsq for PGEN). Allele frequency, sample size and INFO score, if applicable, are computed using only non-missing samples for each phenotype. These are followed by the estimated effect sizes (on original scale), standard errors, chi-square test statistics and -\\log_{10} p-value. If option --write-samples was used, IDs of samples used for each trait will be written in files file_ phenotype1_name .regenie.ids,...,file_ phenotypeP_name .regenie.ids (tab separated, no header). Burden testing Starting from version 2.0, Step 2 of regenie now provides burden testing functionality. More specifically, a user can combine genetic variants within a gene (or region), using functional annotations, into a single combined 'mask' genotype, that can be tested for association using the same testing options as with single variants. Input Option Argument Type Description --anno-file FILE Required File with variant annotations for each set --set-list FILE Required File listing variant sets --extract-sets FILE Optional Inclusion file that lists IDs of variant sets to keep --exclude-sets FILE Optional Exclusion file that lists IDs of variant sets to remove --extract-setlist STRING Optional Comma-separated list of variant sets to keep --exclude-setlist STRING Optional Comma-separated list of variant sets to remove --aaf-file FILE Optional File with variant AAF to use when building masks (instead of AAF estimated from sample) --mask-def FILE Required File with mask definitions using the annotations defined in --anno-labels Annotation input files The following files are used to define variant sets and functional annotations which will be used to generate masks. Annotation file 1:55039839:T:C PCSK9 LoF 1:55039842:G:A PCSK9 missense . This file defines functional annotations for variants. It is designed to accommodate for variants with separate annotations for different sets/genes. Each line contains the variant name, the set/gene name and a single annotation category (space/tab separated). Variants not in this file will be assigned to a default \"NULL\" category. A maximum of 63 annotation categories (+NULL category) is allowed. For gene sets, tools you can use to obtain variant annotations per transcripts are snpEFF or VEP . To obtain a single annotation per gene, you could choose the most deleterious functional annotation across the gene transcripts or alternatively use the canonical transcript (note that its definition can vary across software). We have implemented an extended 4-column format of the annotation file which also categorizes sets into domains (e.g. for gene sets, these would correspond to gene domains). 1:55039839:T:C PCSK9 Prodomain LoF 1:55039842:G:A PCSK9 Prodomain missense . Masks will be generated for each domain (maximum of 8 per set/gene) in addition to a mask combining across all domains. Variants can only be assigned to a single domain for each set/gene. Set list file This file lists variants within each set/gene to use when building masks. Each line contains the set/gene name followed by a chromosome and physical position for the set/gene, then by a comma-separated list of variants included in the set/gene. A1BG 19 58346922 19:58346922:C:A,19:58346924:G:A,... A1CF 10 50806630 10:50806630:A:G,10:50806630:A:AT,... . Variants in the same set must belong to the chromosome specified in the 2nd column. Set inclusion/exclusion file format The file must have a single column of set/gene names corresponding to those in the set list file. PIGP ZBTB38 . AAF file (optional) Both functional annotations and alternative allele frequency (AAF) cutoffs are used when building masks (e.g. only considering LoF sites where AAF is below 1%). By default, the AAF for each variant is computed from the sample but alternatively, the user can specify variant AAFs using this file. Each line contains the variant name followed by its AAF (it should correspond to ALT allele used in the genetic data input). 7:6187101:C:T 1.53918207864341e-05 7:6190395:C:A 2.19920388819247e-06 . Mask definitions Mask file This file specifies which annotation categories should be combined into masks. Each line contains a mask name followed by a comma-seperated list of categories included in the mask (i.e. union is taken over categories). For example below, Mask1 uses only LoF variants and Mask2 uses LoF and missense annotated variants. Mask1 LoF Mask2 LoF,missense . AAF cutoffs Option --aaf-bins specifies the AAF upper bounds used to generate masks. By default, a mask based on singleton sites are always included. For example, --aaf-bins 0.01,0.05 will generate 3 masks for AAFs in [0,0.01], [0,0.05] and singletons. LOVO scheme The leave-one-variant-out (LOVO) scheme takes all sites goint into a mask, and builds LOVO masks by leaving out one variant at a time from the full set of sites. The mask including all sites will also be computed. The argument for --mask-lovo is a comma-separated list which consists of the set/gene name, the mask name, and the AAF cutoff (either 'singleton' or a double in (0,1)). If using a 4-column annotation file, then --mask-lovo should have the set name, the domain name, the mask name, and the AAF cutoff. Writing mask files Masks built in regenie can be written to PLINK bed format. If the input genetic data contains dosages, the masks dosages will be converted to hard-calls prior to being written to file and these hard-calls will be used for the association testing. The PLINK bed file is written using 'ref-last' encoding (i.e. REF allele is listed last in the bim file). Note that this cannot be used with the LOVO scheme. Options Option Argument Type Description --aaf-bins FLOAT,...,FLOAT Optional comma-separated list of AAF upper bounds to use when building masks [default is a single cutoff of 1%] --build-mask STRING Optional build masks using the maximum number of ALT alleles across sites ( 'max' ; the default), or the sum of ALT alleles ( 'sum' ), or thresholding the sum to 2 ( 'comphet' ) --singleton-carrier FLAG Optional to define singletons as variants with a single carrier in the sample (rather than alternative allele count=1) --write-mask FLAG Optional write mask to PLINK bed format (does not work when building masks with 'sum') --skip-test FLAG Optional to skip computing association tests after building masks and writing them to file --mask-lovo STRING Optional to perform LOVO scheme --check-burden-files FLAG Optional to check the concordance between annotation, set list and mask files [see below ] --strict-check-burden FLAG Optional to exit early if the annotation, set list and mask definition files dont agree [see below ] Three rules can be used to build masks with --build-mask as shown in diagram below, where the last rule comphet applies a threshold of 2 to the mask from the sum rule. Output With --out file Results are written in separate files for each phenotype file_ phenotype1_name .regenie,...,file_ phenotypeP_name .regenie with the same output format mentioned above . Additionally, a header line is included (starting with ## ) which contains mask definition information. Masks will have name set_name . mask_name . AAF_cutoff with the chromosome and physical position having been defined in the set list file, and the reference allele being ref , and the alternate allele corresponding to mask_name . AAF_cutoff . When using --mask-lovo , the mask name will be the same as above but have suffix _ variant_name to specify the variant which was excluded when building the mask. With --build-mask sum , the reported mask AAF corresponds to the average AAF across sites included in the mask. If using --write-mask , the masks will be saved to file_masks.{bed,bim,fam} . Example run Using Step 1 results from the Step 1 command above , we use the following command to build and test masks in Step 2 ./regenie \\ --step 2 \\ --bed example/example_3chr \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --bt \\ --remove example/fid_iid_to_remove.txt \\ --firth --approx \\ --pred fit_bin_out_pred.list \\ --anno-file example/example_3chr.annotations \\ --set-list example/example_3chr.setlist \\ --mask-def example/example_3chr.masks \\ --aaf-bins 0.1,0.05 \\ --write-mask \\ --bsize 200 \\ --out test_bin_out_firth For each set, this will produce masks using 3 AAF cutoffs (singletons, 5% and 10% AAF). The masks are written to PLINK bed file (in test_bin_out_firth_masks.{bed,bim,fam} ) and tested for association with each binary trait using Firth approximate test (summary stats in test_bin_out_firth_ phenotype_name .regenie ). Note that the test uses the whole genome regression LOCO PRS from Step 1 of regenie (specified by --pred ). Checking input files To assess the concordance between the input files for building masks, you can use --check-burden-files which will generate a report in file_masks_report.txt containing: for each set, the list the variants in the set-list file which are unrecognized (not genotyped or not present in annotation file for the set) for each mask, the list of annotations in the mask definition file which are not in the annotation file Additionally, you can use --strict-check-burden to enforce full agreement between the three files (if not, program will terminate) : all genotyped variants in the set list file must be in the annotation file (for the corresponding set) all annotations in the mask definition file must be present in the annotation file","title":"Documentation"},{"location":"options/#getting-started","text":"To run regenie , use the command ./regenie on the command line, followed by options and flags as needed. To get a full list of options use ./regenie --help The directory examples/ contains some small example files that are useful when getting started. A test run on a set of binary traits can be achieved by the following 2 commands. In Step 1 , the whole genome regression model is fit to the traits, and a set of genomic predictions are produced as output ./regenie \\ --step 1 \\ --bed example/example \\ --exclude example/snplist_rm.txt \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 100 \\ --bt --lowmem \\ --lowmem-prefix tmp_rg \\ --out fit_bin_out In Step 2 , a set of imputed SNPs are tested for association using a Firth logistic regression model ./regenie \\ --step 2 \\ --bgen example/example.bgen \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --remove example/fid_iid_to_remove.txt \\ --bsize 200 \\ --bt \\ --firth --approx \\ --pThresh 0.01 \\ --pred fit_bin_out_pred.list \\ --out test_bin_out_firth One of the output files from these two commands is included in the example/ directory and you can check they are the same using the following command (the message should be printed out) cmp test_bin_out_firth_Y1.regenie example/example.test_bin_out_firth_Y1.regenie \\ echo Files are identical","title":"Getting started"},{"location":"options/#basic-options","text":"","title":"Basic options"},{"location":"options/#input","text":"Option Argument Type Description --bgen, --bed, --pgen FILE Required Input genetic data file. Either BGEN file eg. file.bgen , or bed/bim/fam prefix that assumes file.bed , file.bim , file.fam exist, or pgen/pvar/psam prefix that assumes file.pgen , file.pvar , file.psam exist --sample FILE Optional Sample file corresponding to input BGEN file --ref-first FLAG Optional Specify to use the first allele as the reference allele for BGEN or PLINK bed/bim/fam file input [default is to use the last allele as the reference] --keep FILE Optional Inclusion file that lists individuals to retain in the analysis --remove FILE Optional Exclusion file that lists individuals to remove from the analysis --extract FILE Optional Inclusion file that lists IDs of variants to keep --exclude FILE Optional Exclusion file that lists IDs of variants to remove --phenoFile FILE Required Phenotypes file --phenoCol STRING Optional Use for each phenotype you want to include in the analysis --phenoColList STRING Optional Comma separated list of phenotypes to include in the analysis --covarFile FILE Optional Covariates file --covarCol STRING Optional Use for each covariate you want to include in the analysis --covarColList STRING Optional Comma separated list of covariates to include in the analysis --catCovarList STRING Optional Comma separated list of categorical covariates to include in the analysis --pred FILE Optional File containing predictions from Step 1 (see Overview). This is required for --step 2 Note: Parameter expansion can be used when specifying phenotypes/covariates (e.g. --covarCol PC{1:10} ).","title":"Input"},{"location":"options/#genetic-data-file-format","text":"regenie can read BGEN files, bed/bim/fam files or pgen/psam/pvar files in Step 1 and Step 2. The BGEN file format is described here . The bed/bim/fam file format is described here . The pgen/pvar/psam file format is described here . Tools useful for genetic data file format conversion are : PLINK , QCTOOL , BCFTOOLS . Step 2 of regenie can be sped up with BGEN files by using v1.2 format with 8 bits encoding (genotype file can be generated with PLINK2 using option --export bgen-1.2 'bits=8' ) as well as having an accompanying .bgi index file (a useful tool to create such file is bgenix which is part of the BGEN library). To include X chromosome genotypes in step 1 and/or step 2, males should be coded as diploid so that their genotypes are 0/2. This can be done in PLINK by setting the sex of all individuals to female before generating the genotype file. Chromosome values of 23 (for human analyses), X, XY, PAR1 and PAR2 are all acceptable and will be collapsed into a single chromosome.","title":"Genetic data file format"},{"location":"options/#sample-inclusionexclusion-file-format","text":"2 2 7 7 . No header. Each line starts with individual FID IID. Space/tab separated. Samples listed in the file that are not in bgen/bed/pgen file are ignored.","title":"Sample inclusion/exclusion file format"},{"location":"options/#variant-inclusionexclusion-file-format","text":"20 31 . No header. Each line must start with variant ID (if there are additional columns, file must be space/tab separated). Variants listed in this file that are not in bgen/bed/pgen file are ignored.","title":"Variant inclusion/exclusion file format"},{"location":"options/#covariate-file-format","text":"FID IID V1 V2 V3 1 1 1.46837294454993 1.93779743016325 0.152887004505393 2 2 -1.2234390803815 -1.63408619199948 -0.190201446835255 3 3 0.0711531925667286 0.0863906292357564 0.14254739715665 . Line 1 : Header with FID, IID and C covariate names. Followed by lines of C+2 values. Space/tab separated. Each line contains individual FID and IID followed by C covariate values. Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. No missing values are allowed. If --step 2 is specified, then the covariate file should be the same as that used in Step 1.","title":"Covariate file format"},{"location":"options/#phenotype-file-format","text":"FID IID Y1 Y2 1 1 1.64818554321186 2.2765234736685 2 2 -2.67352013711554 -1.53680421614647 3 3 0.217542851471485 0.437289912695016 . Line 1 : Header with FID, IID and P phenotypes names. Followed by lines of P+2 values. Space/tab separated. Each line contains individual FID and IID followed by P phenotype values (for binary traits, must be coded as 0=control, 1=case, NA=missing unless using --1 ). Samples listed in this file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in this file are removed from the analysis. Missing values must be coded as NA. With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait . In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes, use option --strict in Step 1 and 2.","title":"Phenotype file format"},{"location":"options/#predictions-file-format","text":"Running --step 1 --out foo will produce A set of files containing genomic predictions for each phenotype from Step 1 (see Output section below). A file called foo_pred.list listing the locations of the prediction files. The file list is needed as an input file when using --step 2 via the --pred option. It has one line per phenotype (in any order) that specifies the name of the phenotype and its corresponding prediction file name. Each phenotype must have exactly one prediction file and phenotype names must match with those in the phenotype file. Phenotypes in this file not included in the analysis are ignored. Each prediction file contains the genetic predictions for the phenotype (space separated). Line 1 starts with 'FID_IID' followed by $N$ sample identifiers. It is followed by 23 lines containing the genetic predictions for each chromosome (sex chromosomes are collapsed into chromosome 23). More specifically, each line has $N+1$ values which are the chromosome number followed by the $N$ leave-one chromosome out (LOCO) predictions for each individual. Samples in this file not in the bed/pgen/bgen input file are ignored. Genotyped samples not present in this file will be ignored in the analysis of the corresponding trait. Samples with missing LOCO predictions must have their corresponding phenotype value set to missing.","title":"Predictions file format"},{"location":"options/#options","text":"Option Argument Type Description --step INT Required specify step for the regenie run (see Overview) [argument can be 1 or 2 ] --bt FLAG Optional specify that traits are binary with 0=control,1=case,NA=missing (default is quantitative) -1,--cc12 FLAG Optional specify to use 1/2/NA encoding for binary traits (1=control,2=case,NA=missing) --bsize INT Required size of the genotype blocks --cv INT Optional number of cross validation (CV) folds [default is 5] --loocv FLAG Optional flag to use leave-one out cross validation --lowmem FLAG Optional flag to reduce memory usage by writing level 0 predictions to disk (details below). This is very useful if the number of traits is large (e.g. greater than 10) --lowmem-prefix FILE PREFIX Optional prefix where to temporarily write the level 0 predictions --split-l0 PREFIX,N Optional split level 0 across N jobs and set prefix of output files of level 0 predictions --run-l0 FILE,K Optional run level 0 for job K in --run-l1 FILE Optional run level 1 specifying the master file from '--split-l0' --keep-l0 FLAG Optional avoid deleting the level 0 predictions written on disk after fitting the level 1 models --print-prs FLAG Optional flag to print whole genome predictions (i.e. PRS) without using LOCO scheme --force-step1 FLAG Optional flag to run step 1 when 1M variants are used (not recommened) --nb INT Optional number of blocks (determined from block size if not provided) --strict FLAG Optional flag to removing samples with missing data at any of the phenotypes --ignore-pred FLAG Optional skip reading the file specified by --pred (corresponds to simple linear/logistic regression) --use-prs FLAG Optional flag to use whole genome PRS in --pred (this is output in step 1 when using --print-prs ) --gz FLAG Optional flag to output files in compressed gzip format (LOCO prediction files in step 1 and association results files in step 2) [this only works when compiling with Boost Iostream library (see Install tab)] . --force-impute FLAG Optional flag to keep and impute missing observations for QTs in step 2 --write-samples FLAG Optional flag to write sample IDs for those kept in the analysis for each trait in step 2 --print-pheno FLAG Optional flag to write phenotype name in the first line of the sample ID files when using --write-samples --firth FLAG Optional specify to use Firth likelihood ratio test (LRT) as fallback for p-values less than threshold --approx FLAG Optional flag to use approximate Firth LRT for computational speedup (only works when option --firth is used) --firth-se FLAG Optional flag to compute SE based on effect size and LRT p-value when using Firth correction (instead of based on Hessian of unpenalized log-likelihood) --spa FLAG Optional specify to use Saddlepoint approximation as fallback for p-values less than threshold --pThresh FLOAT Optional P-value threshold below which to apply Firth/SPA correction [default is 0.05] --test STRING Optional specify to carry out dominant or recessive test [default is additive; argument can be dominant or recessive ] --chr INT Optional specify which chromosomes to test in step 2 (use for each chromosome to include) --chrList STRING Optional Comma separated list of chromosomes to test in step 2 --range STRING Optional specify chromosome region for variants to test in step 2 [format=CHR:MINPOS-MAXPOS] --minMAC FLOAT Optional flag to specify the minimum minor allele count (MAC) when testing variants [default is 5]. Variants with lower MAC are ignored. --minINFO FLOAT Optional flag to specify the minimum imputation info score (IMPUTE/MACH R^2) when testing variants. Variants with lower info score are ignored. --nauto INT Optional number of autosomal chromosomes (for non-human studies) [default is 22] --maxCatLevels INT Optional maximum number of levels for categorical covariates (for non-human studies) [default is 10] --niter INT Optional maximum number of iterations for logistic regression [default is 30] --maxstep-null INT Optional maximum step size for logistic model with Firth penalty under the null [default is 25] --maxiter-null INT Optional maximum number of iterations for logistic model with Firth penalty under the null [default is 1000] --threads INT Optional number of computational threads to use [default=all] --debug FLAG Optional debug flag (for use by developers) --verbose FLAG Optional verbose screen output --help FLAG Optional Prints usage and options list to screen When step 1 of regenie is run in low memory mode (i.e. using --lowmem ), temporary files are created on disk (using --lowmem-prefix tmp_prefix determines where the files are written [as in tmp_prefix_l0_Y1 ,..., tmp_prefix_l0_YP for P phenotypes]). If the prefix is not specified, the default is to use the prefix specified by --out (see below). These are automatically deleted at the end of the program (unless the run was not successful in which case the user would need to delete the files) See the Wiki page for more details on how to run the level 0 models for Step 1 of regenie in parallel.","title":"Options"},{"location":"options/#output","text":"Option Argument Type Description --out FILE PREFIX Required Output files that depends on --step A log file file.log of the output is generated. Using --step 1 --out file For the P phenotypes, files file_1.loco ,..., file_P.loco are output with the per-chromosome LOCO predictions as rows of the files. If option --gz was used, the files will be compressed in gzip format and have extension .loco.gz . Genotyped individuals specified using option --remove are excluded from this file. Individuals with missing phenotype values kept in the analysis are included in the file and have their predictions set to missing. The list of blup files needed for step 2 (association testing) is written to file_pred.list . If using --print-prs , files files_1.prs ,..., files_P.prs will be written with the whole genome predictions (i.e. PRS) without using LOCO scheme (similar format as the .loco files). The list of these files is written to file_prs.list and can be used in step 2 with --pred and specifying flag --use-prs . Note that as these are not obtained using a LOCO scheme, association tests could suffer from proximal contamination. Using --step 2 --out file By default, results are written in separate files for each phenotype file_ phenotype1_name .regenie,...,file_ phenotypeP_name .regenie . Each file has one line per SNP along with a header line. If option --gz was used, the files will be compressed in gzip format and have extension .regenie.gz . The entries of each row specify chromosome, posistion, ID, reference allele (allele 0), alternative allele (allele 1), frequency of the alternative allele, sample size and the test performed (additive/dominant/recessive). With BGEN/PGEN files with dosages, the imputation INFO score is provided (IMPUTE info score for BGEN and Mach Rsq for PGEN). Allele frequency, sample size and INFO score, if applicable, are computed using only non-missing samples for each phenotype. These are followed by the estimated effect sizes (on original scale), standard errors, chi-square test statistics and -\\log_{10} p-value. If option --write-samples was used, IDs of samples used for each trait will be written in files file_ phenotype1_name .regenie.ids,...,file_ phenotypeP_name .regenie.ids (tab separated, no header).","title":"Output"},{"location":"options/#burden-testing","text":"Starting from version 2.0, Step 2 of regenie now provides burden testing functionality. More specifically, a user can combine genetic variants within a gene (or region), using functional annotations, into a single combined 'mask' genotype, that can be tested for association using the same testing options as with single variants.","title":"Burden testing"},{"location":"options/#input_1","text":"Option Argument Type Description --anno-file FILE Required File with variant annotations for each set --set-list FILE Required File listing variant sets --extract-sets FILE Optional Inclusion file that lists IDs of variant sets to keep --exclude-sets FILE Optional Exclusion file that lists IDs of variant sets to remove --extract-setlist STRING Optional Comma-separated list of variant sets to keep --exclude-setlist STRING Optional Comma-separated list of variant sets to remove --aaf-file FILE Optional File with variant AAF to use when building masks (instead of AAF estimated from sample) --mask-def FILE Required File with mask definitions using the annotations defined in --anno-labels","title":"Input"},{"location":"options/#annotation-input-files","text":"The following files are used to define variant sets and functional annotations which will be used to generate masks.","title":"Annotation input files"},{"location":"options/#annotation-file","text":"1:55039839:T:C PCSK9 LoF 1:55039842:G:A PCSK9 missense . This file defines functional annotations for variants. It is designed to accommodate for variants with separate annotations for different sets/genes. Each line contains the variant name, the set/gene name and a single annotation category (space/tab separated). Variants not in this file will be assigned to a default \"NULL\" category. A maximum of 63 annotation categories (+NULL category) is allowed. For gene sets, tools you can use to obtain variant annotations per transcripts are snpEFF or VEP . To obtain a single annotation per gene, you could choose the most deleterious functional annotation across the gene transcripts or alternatively use the canonical transcript (note that its definition can vary across software). We have implemented an extended 4-column format of the annotation file which also categorizes sets into domains (e.g. for gene sets, these would correspond to gene domains). 1:55039839:T:C PCSK9 Prodomain LoF 1:55039842:G:A PCSK9 Prodomain missense . Masks will be generated for each domain (maximum of 8 per set/gene) in addition to a mask combining across all domains. Variants can only be assigned to a single domain for each set/gene.","title":"Annotation file"},{"location":"options/#set-list-file","text":"This file lists variants within each set/gene to use when building masks. Each line contains the set/gene name followed by a chromosome and physical position for the set/gene, then by a comma-separated list of variants included in the set/gene. A1BG 19 58346922 19:58346922:C:A,19:58346924:G:A,... A1CF 10 50806630 10:50806630:A:G,10:50806630:A:AT,... . Variants in the same set must belong to the chromosome specified in the 2nd column.","title":"Set list file"},{"location":"options/#set-inclusionexclusion-file-format","text":"The file must have a single column of set/gene names corresponding to those in the set list file. PIGP ZBTB38 .","title":"Set inclusion/exclusion file format"},{"location":"options/#aaf-file-optional","text":"Both functional annotations and alternative allele frequency (AAF) cutoffs are used when building masks (e.g. only considering LoF sites where AAF is below 1%). By default, the AAF for each variant is computed from the sample but alternatively, the user can specify variant AAFs using this file. Each line contains the variant name followed by its AAF (it should correspond to ALT allele used in the genetic data input). 7:6187101:C:T 1.53918207864341e-05 7:6190395:C:A 2.19920388819247e-06 .","title":"AAF file (optional)"},{"location":"options/#mask-definitions","text":"","title":"Mask definitions"},{"location":"options/#mask-file","text":"This file specifies which annotation categories should be combined into masks. Each line contains a mask name followed by a comma-seperated list of categories included in the mask (i.e. union is taken over categories). For example below, Mask1 uses only LoF variants and Mask2 uses LoF and missense annotated variants. Mask1 LoF Mask2 LoF,missense .","title":"Mask file"},{"location":"options/#aaf-cutoffs","text":"Option --aaf-bins specifies the AAF upper bounds used to generate masks. By default, a mask based on singleton sites are always included. For example, --aaf-bins 0.01,0.05 will generate 3 masks for AAFs in [0,0.01], [0,0.05] and singletons.","title":"AAF cutoffs"},{"location":"options/#lovo-scheme","text":"The leave-one-variant-out (LOVO) scheme takes all sites goint into a mask, and builds LOVO masks by leaving out one variant at a time from the full set of sites. The mask including all sites will also be computed. The argument for --mask-lovo is a comma-separated list which consists of the set/gene name, the mask name, and the AAF cutoff (either 'singleton' or a double in (0,1)). If using a 4-column annotation file, then --mask-lovo should have the set name, the domain name, the mask name, and the AAF cutoff.","title":"LOVO scheme"},{"location":"options/#writing-mask-files","text":"Masks built in regenie can be written to PLINK bed format. If the input genetic data contains dosages, the masks dosages will be converted to hard-calls prior to being written to file and these hard-calls will be used for the association testing. The PLINK bed file is written using 'ref-last' encoding (i.e. REF allele is listed last in the bim file). Note that this cannot be used with the LOVO scheme.","title":"Writing mask files"},{"location":"options/#options_1","text":"Option Argument Type Description --aaf-bins FLOAT,...,FLOAT Optional comma-separated list of AAF upper bounds to use when building masks [default is a single cutoff of 1%] --build-mask STRING Optional build masks using the maximum number of ALT alleles across sites ( 'max' ; the default), or the sum of ALT alleles ( 'sum' ), or thresholding the sum to 2 ( 'comphet' ) --singleton-carrier FLAG Optional to define singletons as variants with a single carrier in the sample (rather than alternative allele count=1) --write-mask FLAG Optional write mask to PLINK bed format (does not work when building masks with 'sum') --skip-test FLAG Optional to skip computing association tests after building masks and writing them to file --mask-lovo STRING Optional to perform LOVO scheme --check-burden-files FLAG Optional to check the concordance between annotation, set list and mask files [see below ] --strict-check-burden FLAG Optional to exit early if the annotation, set list and mask definition files dont agree [see below ] Three rules can be used to build masks with --build-mask as shown in diagram below, where the last rule comphet applies a threshold of 2 to the mask from the sum rule.","title":"Options"},{"location":"options/#output_1","text":"With --out file Results are written in separate files for each phenotype file_ phenotype1_name .regenie,...,file_ phenotypeP_name .regenie with the same output format mentioned above . Additionally, a header line is included (starting with ## ) which contains mask definition information. Masks will have name set_name . mask_name . AAF_cutoff with the chromosome and physical position having been defined in the set list file, and the reference allele being ref , and the alternate allele corresponding to mask_name . AAF_cutoff . When using --mask-lovo , the mask name will be the same as above but have suffix _ variant_name to specify the variant which was excluded when building the mask. With --build-mask sum , the reported mask AAF corresponds to the average AAF across sites included in the mask. If using --write-mask , the masks will be saved to file_masks.{bed,bim,fam} .","title":"Output"},{"location":"options/#example-run","text":"Using Step 1 results from the Step 1 command above , we use the following command to build and test masks in Step 2 ./regenie \\ --step 2 \\ --bed example/example_3chr \\ --covarFile example/covariates.txt \\ --phenoFile example/phenotype_bin.txt \\ --bt \\ --remove example/fid_iid_to_remove.txt \\ --firth --approx \\ --pred fit_bin_out_pred.list \\ --anno-file example/example_3chr.annotations \\ --set-list example/example_3chr.setlist \\ --mask-def example/example_3chr.masks \\ --aaf-bins 0.1,0.05 \\ --write-mask \\ --bsize 200 \\ --out test_bin_out_firth For each set, this will produce masks using 3 AAF cutoffs (singletons, 5% and 10% AAF). The masks are written to PLINK bed file (in test_bin_out_firth_masks.{bed,bim,fam} ) and tested for association with each binary trait using Firth approximate test (summary stats in test_bin_out_firth_ phenotype_name .regenie ). Note that the test uses the whole genome regression LOCO PRS from Step 1 of regenie (specified by --pred ).","title":"Example run"},{"location":"options/#checking-input-files","text":"To assess the concordance between the input files for building masks, you can use --check-burden-files which will generate a report in file_masks_report.txt containing: for each set, the list the variants in the set-list file which are unrecognized (not genotyped or not present in annotation file for the set) for each mask, the list of annotations in the mask definition file which are not in the annotation file Additionally, you can use --strict-check-burden to enforce full agreement between the three files (if not, program will terminate) : all genotyped variants in the set list file must be in the annotation file (for the corresponding set) all annotations in the mask definition file must be present in the annotation file","title":"Checking input files"},{"location":"overview/","text":"Overview This page provides an overview of the models and methods implemented in regenie . A full description is given in our BioRxiv pre-print . regenie carries out genome-wide association tests for both quantitative and binary (case-control) phenotypes. It is designed to handle A large number of samples. For example, it is ideally suited to the UK Biobank dataset with 500,000 samples. A combination of genetic data from a micro-array, imputation and exome sequencing. A large number of either quantitative traits (QTs) or binary (case-control) traits (BTs) Accounting for a set of covariates An overview of the regenie method is provided in the figure below. Essentially, regenie is run in 2 steps: In the first step a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. In the second step, a larger set of genetic markers (e.g. imputed markers) are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1, using a leave one chromosome out (LOCO) scheme, that avoids proximal contamination. Step 1 : Whole genome model In Step 1 a whole genome regression model is fit at a subset of the total set of available genetic markers. These are typically a set of several hundred thousand ( M ) common markers from a micro-array. Ridge regression (level 0) regenie reads in the M markers in blocks of B consecutive markers ( --bsize option). In each block, a set of ridge regression predictors are calculated for a small range of J shrinkage parameters \\{\\tau_1,\\ldots, \\tau_J\\} (using --l0 option [default is 5]) . For a block of SNPs in a N\\times B matrix X and N\\times 1 phenotype vector Y we calculate J predictors X\\widehat{\\beta}_1 \\ldots, X\\widehat{\\beta}_J where \\widehat{\\beta}_j = (X^TX+\\tau_j I)^{-1}X^T Y The idea behind using a range of shrinkage values is to capture the unknown number and size of truly associated genetic markers within each window. The ridge regression takes account of Linkage disequilibrium (LD) within each block. These predictors are stored in place of the genetic markers in matrix W , providing a large reduction in data size. For example, if M=500,000 and B=1,000 and J=5 shrinkage parameters are used, then the reduced dataset will have JM/B=2,500 predictors. Ridge regression is used in this step for both quantitative and binary traits. Cross-validation (level 1) The predictors generated by the ridge regression step will all be positively correlated with the phenotype. Thus, it is important to account for that correlation when building a whole genome wide regression model. When analyzing a quantitative trait we use a second level of ridge regression on the full set of JM/B predictors in W . This approach is inspired by the method of stacked regressions [1]. We fit the ridge regression for a range of shrinkage parameters ( --l1 option) and choose a single best value using K-fold cross validation scheme. This assesses the predictive performance of the model using held out sets of data, and aims to control any over-fitting induced by using the first level of ridge regression to derive the predictors. In other words, we fit the model Y = W\\alpha + \\epsilon where \\alpha is estimated as \\widehat{\\alpha} = (W^TW + \\phi\\,I)^{-1}W^TY and the parameter \\phi is chosen via K-fold cross-validation. For binary traits, we use a logistic ridge regression model to combine the predictors in W \\text{logit}(p) = \\mu + W\\alpha where p is the probability of being a case and \\mu captures the effects of non-genetic covariates. Genetic predictors and LOCO Once \\alpha has been estimated we can construct the genetic prediction Z = W\\widehat{\\alpha} Also, since each column of the matrix W will be associated with a chromosome we can can also construct a genetic prediction ignoring any one chromosome, by simply ignoring those columns when calculating the prediction. This is known as the Leave One Chromosome Out (LOCO) approach. These LOCO predictions are valuable at Step 2 of regenie when each marker is tested for associated (see below). For binary traits, it is the linear predictor in a logistic regression model using LOCO that is saved, and used as an offset when fitting logistic regression models to test for association. Multiple phenotypes The dimension reduction step using ridge regression can be used very efficiently to model multiple phenotypes at once. The ridge regression equations for a block of SNPs in a N\\times B matrix X and a single phenotype in a N\\times 1 matrix Y take the form \\widehat{\\beta} = AY where A = (X^TX+\\tau I)^{-1}X^T does not depend on Y If instead P phenotypes are stored in columns of a N\\times P matrix Y , then the matrix A can be applied jointly to calculate the matrix of estimates \\widehat{\\beta} = AY , and this can take advantage of parallel linear algebra implementations in the Eigen matrix library. Covariates Covariates, such as age and sex and batch effect variables can be included in the regenie model. For quantitative traits, any covariates are regressed out of phenotypes and genotypes before fitting the model. For binary traits, we fit a null model with only covariates, and use predictions from that model as an offset when fitting the logistic regression model. Step 2 : Association testing In Step 2 a larger set of markers are tested for association with the trait (or traits). As with Step 1, these markers are also read in blocks of B markers, and tested for association. This avoids having to have all markers stored in memory at once. Quantitative traits For quantitative traits, we use a linear regression model for association testing. Covariates are regressed out of the phenotypes and genetic markers. The LOCO predictions from Step 1 are removed from the phenotypes. Linear regression is then used to test association of the residualized phenotype and the genetic marker. Parallel linear algebra operations in the Eigen library are used where possible. Binary traits For binary traits, logistic regression is used to test association of the phenotype and the genetic marker. The logistic regression model includes the LOCO predictions from Step 1 as an offset . Covariates are included in the linear predictor in the usual way. When the case-control ratio is imbalanced, standard association tests don't control Type I error well at rare genetic markers. regenie has two options to handle this Firth logistic regression Standard maximum likelihood estimates are generally biased. The Firth correction [2] removes much of the bias, and results in better calibrated test statistics. The correction involves adding a penalty term to the log-likelihood, \\widetilde{l}(\\theta) = l(\\theta) + {1 \\over 2} \\log I|\\theta| where the penalty term corresponds to the use of Jeffrey's Prior. This prior has the effect of shrinking the effect size towards zero. regenie uses a Firth correction when the p-value from the standard logistic regression test is below a threshold (default 0.05). It also includes a novel, accurate and fast approximate Firth correction which is ~60x faster than the exact Firth correction (see the option --firth ). The p-value reported in regenie is based on a likelihood ratio test (LRT), and we use the Hessian of the log-likelihood without the penalty term to estimate the standard error (SE). This may cause an issue in meta-analyses with rare variants, as the effect size estimate and SE may not match with the LRT p-value. Hence, we added an option --firth-se to report a SE computed instead from the effect size estimate and the LRT p-value. Saddle point approxiation (SPA) test The SPA test approximates the null distribution of the test statistic by approximating the cumulant generating function of the test statistic, which involves all of the higher order moments [3,4]. This provides a better estimation of the tail probabilities compared to using standard asymptotic theory which relies on the normal approximation and uses only the first two moments of the dsitribution. A tail probability is obtained as \\begin{align*} P&(T < t_{\\text{obs}}) \\approx \\Phi(z), \\text{ where,}\\\\ z &= w + \\frac{1}{w}\\log{\\frac{v}{w}}\\\\ w &= \\text{sign}(\\delta^*)\\sqrt{ 2 [ t_{\\text{obs}}\\, \\delta^* - K(\\delta^*)}],\\, v = \\delta^*\\sqrt{K''(\\delta^*)} \\end{align*} and K(\\delta) is the cumulant generating function of the test statistic and \\delta^* is obtained by using a root-finding algorithm for K'(\\delta)=t_{\\text{obs}} . As this approximation has been found not to work very well for ultra-rare variants, a minimum minor allele count (MAC) is used to filter out these variants before testing (option --minMAC ). Missing Phenotype data With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait. In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes from the analysis, use option --strict in step 1 and 2. This can also be used when analyzing a single trait to only keep individuals with complete data by setting the phenotype values of individuals to remove to NA. Note: imputation is only applied to phenotypes; covariates are not allowed to have missing data. References [1] L. Breiman (1996) Stacked Regressions. Machine Learning, 24, 49-64. [2] D. Firth (1993) Bias reduction of maximum likelihood estimates. Biometrika 80, 27\u201338. [3] R. Butler (2007) Saddlepoint Approximations with Applications. Cambridge University Press. [4] R. Dey et al. (2017) A Fast and Accurate Algorithm to Test for Binary Phenotypes and Its Application to PheWAS.The American Journal of Human Genetics 101, 37\u201349.","title":"Overview"},{"location":"overview/#overview","text":"This page provides an overview of the models and methods implemented in regenie . A full description is given in our BioRxiv pre-print . regenie carries out genome-wide association tests for both quantitative and binary (case-control) phenotypes. It is designed to handle A large number of samples. For example, it is ideally suited to the UK Biobank dataset with 500,000 samples. A combination of genetic data from a micro-array, imputation and exome sequencing. A large number of either quantitative traits (QTs) or binary (case-control) traits (BTs) Accounting for a set of covariates An overview of the regenie method is provided in the figure below. Essentially, regenie is run in 2 steps: In the first step a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. In the second step, a larger set of genetic markers (e.g. imputed markers) are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1, using a leave one chromosome out (LOCO) scheme, that avoids proximal contamination.","title":"Overview"},{"location":"overview/#step-1-whole-genome-model","text":"In Step 1 a whole genome regression model is fit at a subset of the total set of available genetic markers. These are typically a set of several hundred thousand ( M ) common markers from a micro-array.","title":"Step 1 : Whole genome model"},{"location":"overview/#ridge-regression-level-0","text":"regenie reads in the M markers in blocks of B consecutive markers ( --bsize option). In each block, a set of ridge regression predictors are calculated for a small range of J shrinkage parameters \\{\\tau_1,\\ldots, \\tau_J\\} (using --l0 option [default is 5]) . For a block of SNPs in a N\\times B matrix X and N\\times 1 phenotype vector Y we calculate J predictors X\\widehat{\\beta}_1 \\ldots, X\\widehat{\\beta}_J where \\widehat{\\beta}_j = (X^TX+\\tau_j I)^{-1}X^T Y The idea behind using a range of shrinkage values is to capture the unknown number and size of truly associated genetic markers within each window. The ridge regression takes account of Linkage disequilibrium (LD) within each block. These predictors are stored in place of the genetic markers in matrix W , providing a large reduction in data size. For example, if M=500,000 and B=1,000 and J=5 shrinkage parameters are used, then the reduced dataset will have JM/B=2,500 predictors. Ridge regression is used in this step for both quantitative and binary traits.","title":"Ridge regression (level 0)"},{"location":"overview/#cross-validation-level-1","text":"The predictors generated by the ridge regression step will all be positively correlated with the phenotype. Thus, it is important to account for that correlation when building a whole genome wide regression model. When analyzing a quantitative trait we use a second level of ridge regression on the full set of JM/B predictors in W . This approach is inspired by the method of stacked regressions [1]. We fit the ridge regression for a range of shrinkage parameters ( --l1 option) and choose a single best value using K-fold cross validation scheme. This assesses the predictive performance of the model using held out sets of data, and aims to control any over-fitting induced by using the first level of ridge regression to derive the predictors. In other words, we fit the model Y = W\\alpha + \\epsilon where \\alpha is estimated as \\widehat{\\alpha} = (W^TW + \\phi\\,I)^{-1}W^TY and the parameter \\phi is chosen via K-fold cross-validation. For binary traits, we use a logistic ridge regression model to combine the predictors in W \\text{logit}(p) = \\mu + W\\alpha where p is the probability of being a case and \\mu captures the effects of non-genetic covariates.","title":"Cross-validation (level 1)"},{"location":"overview/#genetic-predictors-and-loco","text":"Once \\alpha has been estimated we can construct the genetic prediction Z = W\\widehat{\\alpha} Also, since each column of the matrix W will be associated with a chromosome we can can also construct a genetic prediction ignoring any one chromosome, by simply ignoring those columns when calculating the prediction. This is known as the Leave One Chromosome Out (LOCO) approach. These LOCO predictions are valuable at Step 2 of regenie when each marker is tested for associated (see below). For binary traits, it is the linear predictor in a logistic regression model using LOCO that is saved, and used as an offset when fitting logistic regression models to test for association.","title":"Genetic predictors and LOCO"},{"location":"overview/#multiple-phenotypes","text":"The dimension reduction step using ridge regression can be used very efficiently to model multiple phenotypes at once. The ridge regression equations for a block of SNPs in a N\\times B matrix X and a single phenotype in a N\\times 1 matrix Y take the form \\widehat{\\beta} = AY where A = (X^TX+\\tau I)^{-1}X^T does not depend on Y If instead P phenotypes are stored in columns of a N\\times P matrix Y , then the matrix A can be applied jointly to calculate the matrix of estimates \\widehat{\\beta} = AY , and this can take advantage of parallel linear algebra implementations in the Eigen matrix library.","title":"Multiple phenotypes"},{"location":"overview/#covariates","text":"Covariates, such as age and sex and batch effect variables can be included in the regenie model. For quantitative traits, any covariates are regressed out of phenotypes and genotypes before fitting the model. For binary traits, we fit a null model with only covariates, and use predictions from that model as an offset when fitting the logistic regression model.","title":"Covariates"},{"location":"overview/#step-2-association-testing","text":"In Step 2 a larger set of markers are tested for association with the trait (or traits). As with Step 1, these markers are also read in blocks of B markers, and tested for association. This avoids having to have all markers stored in memory at once.","title":"Step 2 : Association testing"},{"location":"overview/#quantitative-traits","text":"For quantitative traits, we use a linear regression model for association testing. Covariates are regressed out of the phenotypes and genetic markers. The LOCO predictions from Step 1 are removed from the phenotypes. Linear regression is then used to test association of the residualized phenotype and the genetic marker. Parallel linear algebra operations in the Eigen library are used where possible.","title":"Quantitative traits"},{"location":"overview/#binary-traits","text":"For binary traits, logistic regression is used to test association of the phenotype and the genetic marker. The logistic regression model includes the LOCO predictions from Step 1 as an offset . Covariates are included in the linear predictor in the usual way. When the case-control ratio is imbalanced, standard association tests don't control Type I error well at rare genetic markers. regenie has two options to handle this","title":"Binary traits"},{"location":"overview/#firth-logistic-regression","text":"Standard maximum likelihood estimates are generally biased. The Firth correction [2] removes much of the bias, and results in better calibrated test statistics. The correction involves adding a penalty term to the log-likelihood, \\widetilde{l}(\\theta) = l(\\theta) + {1 \\over 2} \\log I|\\theta| where the penalty term corresponds to the use of Jeffrey's Prior. This prior has the effect of shrinking the effect size towards zero. regenie uses a Firth correction when the p-value from the standard logistic regression test is below a threshold (default 0.05). It also includes a novel, accurate and fast approximate Firth correction which is ~60x faster than the exact Firth correction (see the option --firth ). The p-value reported in regenie is based on a likelihood ratio test (LRT), and we use the Hessian of the log-likelihood without the penalty term to estimate the standard error (SE). This may cause an issue in meta-analyses with rare variants, as the effect size estimate and SE may not match with the LRT p-value. Hence, we added an option --firth-se to report a SE computed instead from the effect size estimate and the LRT p-value.","title":"Firth logistic regression"},{"location":"overview/#saddle-point-approxiation-spa-test","text":"The SPA test approximates the null distribution of the test statistic by approximating the cumulant generating function of the test statistic, which involves all of the higher order moments [3,4]. This provides a better estimation of the tail probabilities compared to using standard asymptotic theory which relies on the normal approximation and uses only the first two moments of the dsitribution. A tail probability is obtained as \\begin{align*} P&(T < t_{\\text{obs}}) \\approx \\Phi(z), \\text{ where,}\\\\ z &= w + \\frac{1}{w}\\log{\\frac{v}{w}}\\\\ w &= \\text{sign}(\\delta^*)\\sqrt{ 2 [ t_{\\text{obs}}\\, \\delta^* - K(\\delta^*)}],\\, v = \\delta^*\\sqrt{K''(\\delta^*)} \\end{align*} and K(\\delta) is the cumulant generating function of the test statistic and \\delta^* is obtained by using a root-finding algorithm for K'(\\delta)=t_{\\text{obs}} . As this approximation has been found not to work very well for ultra-rare variants, a minimum minor allele count (MAC) is used to filter out these variants before testing (option --minMAC ).","title":"Saddle point approxiation (SPA) test"},{"location":"overview/#missing-phenotype-data","text":"With QTs, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2 (unless using --force-impute ). With BTs, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait. In Step 2, missing values are dropped when testing each trait. To remove all samples that have missing values at any of the P phenotypes from the analysis, use option --strict in step 1 and 2. This can also be used when analyzing a single trait to only keep individuals with complete data by setting the phenotype values of individuals to remove to NA. Note: imputation is only applied to phenotypes; covariates are not allowed to have missing data.","title":"Missing Phenotype data"},{"location":"overview/#references","text":"[1] L. Breiman (1996) Stacked Regressions. Machine Learning, 24, 49-64. [2] D. Firth (1993) Bias reduction of maximum likelihood estimates. Biometrika 80, 27\u201338. [3] R. Butler (2007) Saddlepoint Approximations with Applications. Cambridge University Press. [4] R. Dey et al. (2017) A Fast and Accurate Algorithm to Test for Binary Phenotypes and Its Application to PheWAS.The American Journal of Human Genetics 101, 37\u201349.","title":"References"},{"location":"performance/","text":"Performance We assessed the performance of regenie against 3 other programs for GWAS on large cohorts. BOLT-LMM Loh et al. (2015) Nature Genetics 47, 284\u2013290 [Software] SAIGE - Zhou et al. (2018) Nature Genetics 50, 1335\u20131341 [Software] fastGWA - Jiang et al. (2019) Nature Genetics 51, 1749\u20131755 [Software] Full details for all the runs are available in our BioRxiv pre-print . Quantitative traits We ran regenie , BOLT-LMM and fastGWA on 3 quantitative phenotypes measured on white British UK Biobank participants (LDL, N=389,189; Body mass index [BMI], N=407,609; and Bilirubin, N=388,303) where testing was performed on 9.8 million imputed SNPs. The Manhattan plots for all three phenotypes (see below) show good agreement between the methods with both regenie and BOLT-LMM resulting in stronger association signals relative to fastGWA at known peaks of association (note that in the plots, the scaling of the y-axis changes above the upper dashed line). We assessed the computational requirements of all three methods using a larger set of 50 quantitative traits from the UK Biobank, looking at computational timings as well as memory usage. For regenie and BOLT LMM, 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and for fastGWA, these SNPs were used to compute the sparse GRM (timing not included). Tests were performed on 11.4M imputed SNPs (step 2). From the table above, regenie was 151x faster than BOLT-LMM in elapsed time for Step 1 and 11.5x faster for Step 2, which translated into $ $30x overall speed-up in terms of elapsed time. In addition, regenie had a maximum memory usage of 12.9 GB, which is mostly due to regenie only reading a small portion of the genotype data at a time, whereas BOLT-LMM required 50GB. regenie was 2.8x faster than fastGWA, but fastGWA is very memory efficient and used only a maximum of 2GB. Binary traits regenie was compared to BOLT-LMM and SAIGE on a set of four binary traits measured on white British UK Biobank participants (coronary artery disease [CAD], N=352,063, case-control ratio=1:11; glaucoma, N=406,927, case-control ratio=1:52; colorectal cancer, N=407,746, case-control ratio=1:97; and thyroid cancer, N=407,746, case-control ratio=1:660) and Step 2 testing was performed on 11.6 million imputed SNPs. A novel and fast approximate Firth correction was used in regenie as well as a SPA correction. As seen in the Manhattan plots below (note that the scaling of the y-axis changes above the upper dashed line), all four approaches show very good agreement for the most balanced trait (CAD; case-control ratio=1:11), but as the fraction of cases decreases BOLT-LMM tends to give inflated test statistics. However both regenie with Firth and SPA corrections, as well as SAIGE, which uses SPA correction, are all robust to this inflation and show similar agreement for the associations detected. We assessed the computational requirements of regenie and SAIGE using a larger set of 50 binary traits from the UK Biobank that have a range of different case-control ratios and distinct missing data patterns. 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and tests were performed on 11.4M imputed SNPs (step 2). In step 1, regenie was run using LOOCV and for two traits SAIGE did not finish as it took longer than the 4-week limit. In step 2, the approximate Firth correction was used in regenie in addition to SPA correction. From the table above, Step 1 of regenie was about 350x faster and required only $40\\%$ of the memory used by SAIGE. In Step 2, regenie Firth and SPA were 2x and 3x faster than SAIGE in CPU time, respectively, but were 21x and 34x faster than SAIGE in elapsed time, respectively, which suggests that regenie makes better use of parallelization in this step. Overall, regenie using Firth correction was 8x faster than SAIGE in CPU hours and 26.8x faster in elapsed time. All runs above were done on the same computing environment (16 virtual CPU cores of a 2.1GHz AMD EPYC 7571 processor, 64GB of memory, and 600GB solid-state disk).","title":"Performance"},{"location":"performance/#performance","text":"We assessed the performance of regenie against 3 other programs for GWAS on large cohorts. BOLT-LMM Loh et al. (2015) Nature Genetics 47, 284\u2013290 [Software] SAIGE - Zhou et al. (2018) Nature Genetics 50, 1335\u20131341 [Software] fastGWA - Jiang et al. (2019) Nature Genetics 51, 1749\u20131755 [Software] Full details for all the runs are available in our BioRxiv pre-print .","title":"Performance"},{"location":"performance/#quantitative-traits","text":"We ran regenie , BOLT-LMM and fastGWA on 3 quantitative phenotypes measured on white British UK Biobank participants (LDL, N=389,189; Body mass index [BMI], N=407,609; and Bilirubin, N=388,303) where testing was performed on 9.8 million imputed SNPs. The Manhattan plots for all three phenotypes (see below) show good agreement between the methods with both regenie and BOLT-LMM resulting in stronger association signals relative to fastGWA at known peaks of association (note that in the plots, the scaling of the y-axis changes above the upper dashed line). We assessed the computational requirements of all three methods using a larger set of 50 quantitative traits from the UK Biobank, looking at computational timings as well as memory usage. For regenie and BOLT LMM, 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and for fastGWA, these SNPs were used to compute the sparse GRM (timing not included). Tests were performed on 11.4M imputed SNPs (step 2). From the table above, regenie was 151x faster than BOLT-LMM in elapsed time for Step 1 and 11.5x faster for Step 2, which translated into $ $30x overall speed-up in terms of elapsed time. In addition, regenie had a maximum memory usage of 12.9 GB, which is mostly due to regenie only reading a small portion of the genotype data at a time, whereas BOLT-LMM required 50GB. regenie was 2.8x faster than fastGWA, but fastGWA is very memory efficient and used only a maximum of 2GB.","title":"Quantitative traits"},{"location":"performance/#binary-traits","text":"regenie was compared to BOLT-LMM and SAIGE on a set of four binary traits measured on white British UK Biobank participants (coronary artery disease [CAD], N=352,063, case-control ratio=1:11; glaucoma, N=406,927, case-control ratio=1:52; colorectal cancer, N=407,746, case-control ratio=1:97; and thyroid cancer, N=407,746, case-control ratio=1:660) and Step 2 testing was performed on 11.6 million imputed SNPs. A novel and fast approximate Firth correction was used in regenie as well as a SPA correction. As seen in the Manhattan plots below (note that the scaling of the y-axis changes above the upper dashed line), all four approaches show very good agreement for the most balanced trait (CAD; case-control ratio=1:11), but as the fraction of cases decreases BOLT-LMM tends to give inflated test statistics. However both regenie with Firth and SPA corrections, as well as SAIGE, which uses SPA correction, are all robust to this inflation and show similar agreement for the associations detected. We assessed the computational requirements of regenie and SAIGE using a larger set of 50 binary traits from the UK Biobank that have a range of different case-control ratios and distinct missing data patterns. 469,336 LD-pruned SNPs were used as model SNPs when fitting the null model (step 1) and tests were performed on 11.4M imputed SNPs (step 2). In step 1, regenie was run using LOOCV and for two traits SAIGE did not finish as it took longer than the 4-week limit. In step 2, the approximate Firth correction was used in regenie in addition to SPA correction. From the table above, Step 1 of regenie was about 350x faster and required only $40\\%$ of the memory used by SAIGE. In Step 2, regenie Firth and SPA were 2x and 3x faster than SAIGE in CPU time, respectively, but were 21x and 34x faster than SAIGE in elapsed time, respectively, which suggests that regenie makes better use of parallelization in this step. Overall, regenie using Firth correction was 8x faster than SAIGE in CPU hours and 26.8x faster in elapsed time. All runs above were done on the same computing environment (16 virtual CPU cores of a 2.1GHz AMD EPYC 7571 processor, 64GB of memory, and 600GB solid-state disk).","title":"Binary traits"},{"location":"recommendations/","text":"Recommendations for UK Biobank analysis regenie is ideally suited for large-scale analyses such as 500K UK Biobank (UKBB) data, where records are available for thousands of phenotypes. We provide below a few guidelines on how to perform such analysis on the UKBB files that all UKBB approved researchers have access to. Pre-processing We will first go over important steps to consider before running regenie . Selection of traits regenie can perform whole genome regression on multiple traits at once, which is where higher computational gains are obtained. As different traits can have distinct missing patterns, regenie uses an imputation scheme to handle missing data. From the real data applications we have performed so far with traits having up to ~20% (for quantitative) and ~5% (for binary) missing observations, our imputation scheme resulted in nearly identical results as from discarding missing observations when analyzing each trait separately (see the BioRxiv pre-print for details). Hence, we recommend to analyze traits in groups that have similar missingness patterns with resonably low amount of missingness ( 15%). The number of phenotypes in a group will affect the computational resources required and the table below shows typical computational requirements based on using 500,000 markers in step 1 split in blocks of 1000 and using blocks of size 200 when testing SNPs in step 2. The estimates are shown when step 1 of regenie is run in low-memory mode so that within-block predictions are temporarily stored on disk (see Documentation). In the following sections, we'll assume traits (let's say binary) and covariates used in the analysis have been chosen and data are in files ukb_phenotypes_BT.txt and ukb_covariates.txt , which follow the format requirement for regenie (see Documentation). Preparing genotype file Step 1 of a regenie run requires a single genotype file as input; we recommend using array genotypes for this step. The UKBB genotype files are split by chromosome, so we recommend using PLINK to merge the files using the following code. NOTE : please change XXX to you own UKBB application ID number rm -f list_beds.txt for chr in {2..22}; do echo ukb_cal_chr${chr}_v2.bed ukb_snp_chr${chr}_v2.bim ukbXXX_int_chr1_v2_s488373.fam list_beds.txt; done plink \\ --bed ukb_cal_chr1_v2.bed \\ --bim ukb_snp_chr1_v2.bim \\ --fam ukbXXX_int_chr1_v2_s488373.fam \\ --merge-list list_beds.txt \\ --make-bed --out ukb_cal_allChrs Exclusion files Quality control (QC) filters can be applied using PLINK2 to filter out samples and markers in the genotype file prior to step 1 of regenie . Note: regenie will throw an error if a low-variance SNP is included in the step 1 run. Hence, the user should run adequate QC filtering prior to running regenie to identify and remove such SNPs. For example, to filter out SNPs with minor allele frequency (MAF) below 1%, minor allele count (MAC) below 100, genotype missingess above 10% and Hardy-Weinberg equilibrium p-value exceeding 10^{-15} , and samples with more than 10% missingness, plink2 \\ --bfile ukb_cal_allChrs \\ --maf 0.01 --mac 100 --geno 0.1 --hwe 1e-15 \\ --mind 0.1 \\ --write-snplist --write-samples --no-id-header \\ --out qc_pass Step 1 We recommend to run regenie using multi-threading (8+ threads) which will decrease the overall runtime of the program. As this step can be quite memory intensive (due to storing block predictions), we recommend to use option --lowmem , where the number of phenotypes analyzed will determine how much disk space is required (see table above). Running step 1 of regenie (by default, all available threads are used) ./regenie \\ --step 1 \\ --bed ukb_cal_allChrs \\ --extract qc_pass.snplist \\ --keep qc_pass.id \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --bsize 1000 \\ --lowmem \\ --lowmem-prefix tmpdir/regenie_tmp_preds \\ --out ukb_step1_BT For P phenotypes analyzed, this will generate a set of $P$ files ending with .loco which contain the genetic predictions using a LOCO scheme that will be needed for step 2, as well as a prediction list file ukb_step1_BT_pred.list , which lists the names of these predictions files and can be used as input for step 2. Step 2 As step 1 and 2 are completely decoupled in regenie , you could either use all the traits for testing in step 2 or select a subset of the traits to perform association testing. Furthermore, you can use the same Step 1 output to test on array, exome or imputed variants; below, we will illustrate testing on imputed variants. Step 2 of regenie can be run in parallel across chromosomes so if you have access to multiple machines, we recommend to split the runs over chromosomes (using 8+ threads). Running regenie tesing on a single chromosome (here chromosome 1) and using the fast Firth correction as fallback, ./regenie \\ --step 2 \\ --bgen ukb_imp_chr1_v3.bgen \\ --ref-first \\ --sample ukbXXX_imp_chr1_v3_s487395.sample \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --firth 0.01 --approx \\ --pred ukb_step1_BT_pred.list \\ --bsize 400 \\ --split \\ --out ukb_step2_BT_chr1 This will create separate association results files for each phenotype as ukb_step2_BT_chr1_*.regenie .","title":"UKBB Analysis"},{"location":"recommendations/#recommendations-for-uk-biobank-analysis","text":"regenie is ideally suited for large-scale analyses such as 500K UK Biobank (UKBB) data, where records are available for thousands of phenotypes. We provide below a few guidelines on how to perform such analysis on the UKBB files that all UKBB approved researchers have access to.","title":"Recommendations for UK Biobank analysis"},{"location":"recommendations/#pre-processing","text":"We will first go over important steps to consider before running regenie .","title":"Pre-processing"},{"location":"recommendations/#selection-of-traits","text":"regenie can perform whole genome regression on multiple traits at once, which is where higher computational gains are obtained. As different traits can have distinct missing patterns, regenie uses an imputation scheme to handle missing data. From the real data applications we have performed so far with traits having up to ~20% (for quantitative) and ~5% (for binary) missing observations, our imputation scheme resulted in nearly identical results as from discarding missing observations when analyzing each trait separately (see the BioRxiv pre-print for details). Hence, we recommend to analyze traits in groups that have similar missingness patterns with resonably low amount of missingness ( 15%). The number of phenotypes in a group will affect the computational resources required and the table below shows typical computational requirements based on using 500,000 markers in step 1 split in blocks of 1000 and using blocks of size 200 when testing SNPs in step 2. The estimates are shown when step 1 of regenie is run in low-memory mode so that within-block predictions are temporarily stored on disk (see Documentation). In the following sections, we'll assume traits (let's say binary) and covariates used in the analysis have been chosen and data are in files ukb_phenotypes_BT.txt and ukb_covariates.txt , which follow the format requirement for regenie (see Documentation).","title":"Selection of traits"},{"location":"recommendations/#preparing-genotype-file","text":"Step 1 of a regenie run requires a single genotype file as input; we recommend using array genotypes for this step. The UKBB genotype files are split by chromosome, so we recommend using PLINK to merge the files using the following code. NOTE : please change XXX to you own UKBB application ID number rm -f list_beds.txt for chr in {2..22}; do echo ukb_cal_chr${chr}_v2.bed ukb_snp_chr${chr}_v2.bim ukbXXX_int_chr1_v2_s488373.fam list_beds.txt; done plink \\ --bed ukb_cal_chr1_v2.bed \\ --bim ukb_snp_chr1_v2.bim \\ --fam ukbXXX_int_chr1_v2_s488373.fam \\ --merge-list list_beds.txt \\ --make-bed --out ukb_cal_allChrs","title":"Preparing genotype file"},{"location":"recommendations/#exclusion-files","text":"Quality control (QC) filters can be applied using PLINK2 to filter out samples and markers in the genotype file prior to step 1 of regenie . Note: regenie will throw an error if a low-variance SNP is included in the step 1 run. Hence, the user should run adequate QC filtering prior to running regenie to identify and remove such SNPs. For example, to filter out SNPs with minor allele frequency (MAF) below 1%, minor allele count (MAC) below 100, genotype missingess above 10% and Hardy-Weinberg equilibrium p-value exceeding 10^{-15} , and samples with more than 10% missingness, plink2 \\ --bfile ukb_cal_allChrs \\ --maf 0.01 --mac 100 --geno 0.1 --hwe 1e-15 \\ --mind 0.1 \\ --write-snplist --write-samples --no-id-header \\ --out qc_pass","title":"Exclusion files"},{"location":"recommendations/#step-1","text":"We recommend to run regenie using multi-threading (8+ threads) which will decrease the overall runtime of the program. As this step can be quite memory intensive (due to storing block predictions), we recommend to use option --lowmem , where the number of phenotypes analyzed will determine how much disk space is required (see table above). Running step 1 of regenie (by default, all available threads are used) ./regenie \\ --step 1 \\ --bed ukb_cal_allChrs \\ --extract qc_pass.snplist \\ --keep qc_pass.id \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --bsize 1000 \\ --lowmem \\ --lowmem-prefix tmpdir/regenie_tmp_preds \\ --out ukb_step1_BT For P phenotypes analyzed, this will generate a set of $P$ files ending with .loco which contain the genetic predictions using a LOCO scheme that will be needed for step 2, as well as a prediction list file ukb_step1_BT_pred.list , which lists the names of these predictions files and can be used as input for step 2.","title":"Step 1"},{"location":"recommendations/#step-2","text":"As step 1 and 2 are completely decoupled in regenie , you could either use all the traits for testing in step 2 or select a subset of the traits to perform association testing. Furthermore, you can use the same Step 1 output to test on array, exome or imputed variants; below, we will illustrate testing on imputed variants. Step 2 of regenie can be run in parallel across chromosomes so if you have access to multiple machines, we recommend to split the runs over chromosomes (using 8+ threads). Running regenie tesing on a single chromosome (here chromosome 1) and using the fast Firth correction as fallback, ./regenie \\ --step 2 \\ --bgen ukb_imp_chr1_v3.bgen \\ --ref-first \\ --sample ukbXXX_imp_chr1_v3_s487395.sample \\ --phenoFile ukb_phenotypes_BT.txt \\ --covarFile ukb_covariates.txt \\ --bt \\ --firth 0.01 --approx \\ --pred ukb_step1_BT_pred.list \\ --bsize 400 \\ --split \\ --out ukb_step2_BT_chr1 This will create separate association results files for each phenotype as ukb_step2_BT_chr1_*.regenie .","title":"Step 2"}]}